{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAT281 - Laboratorio N°10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='p1'></a>\n",
    "## I.- Problema 01\n",
    "\n",
    "\n",
    "<img src=\"https://www.goodnewsnetwork.org/wp-content/uploads/2019/07/immunotherapy-vaccine-attacks-cancer-cells-immune-blood-Fotolia_purchased.jpg\" width=\"360\" height=\"360\" align=\"center\"/>\n",
    "\n",
    "\n",
    "El **cáncer de mama**  es una proliferación maligna de las células epiteliales que revisten los conductos o lobulillos mamarios. Es una enfermedad clonal; donde una célula individual producto de una serie de mutaciones somáticas o de línea germinal adquiere la capacidad de dividirse sin control ni orden, haciendo que se reproduzca hasta formar un tumor. El tumor resultante, que comienza como anomalía leve, pasa a ser grave, invade tejidos vecinos y, finalmente, se propaga a otras partes del cuerpo.\n",
    "\n",
    "El conjunto de datos se denomina `BC.csv`, el cual contine la información de distintos pacientes con tumosres (benignos o malignos) y algunas características del mismo.\n",
    "\n",
    "\n",
    "Las características se calculan a partir de una imagen digitalizada de un aspirado con aguja fina (FNA) de una masa mamaria. Describen las características de los núcleos celulares presentes en la imagen.\n",
    "Los detalles se puede encontrar en [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "\n",
    "Lo primero será cargar el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_palette(\"deep\", desat=.6)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar datos\n",
    "df = pd.read_csv(os.path.join(\"data\",\"BC.csv\"), sep=\",\")\n",
    "df['diagnosis'] = df['diagnosis'] .replace({'M':1,'B':0}) # target \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en la información presentada responda las siguientes preguntas:\n",
    "\n",
    "1. [Realice un análisis exploratorio del conjunto de datos.](#e1)\n",
    "2. [Normalizar las variables numéricas con el método **StandardScaler**.](#e2)\n",
    "3. [Realizar un método de reducción de dimensionalidad visto en clases.](#e3)\n",
    "4. [Aplique al menos tres modelos de clasificación distintos. Para cada uno de los modelos escogidos, realice una optimización de los hiperparámetros. además, calcule las respectivas métricas. Concluya.](#e4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='e1'></a>\n",
    "**1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Análisis exploratorio\n",
    "# se usará esta funcion de la clase de eda\n",
    "def resumen_por_columna(df,cols):\n",
    "    pd_series = df[cols]\n",
    "    \n",
    "    # elementos distintos \n",
    "    l_unique = pd_series.unique()\n",
    "    \n",
    "    # elementos vacios\n",
    "    \n",
    "    l_vacios = pd_series[pd_series.isna()]\n",
    "    \n",
    "    df_info = pd.DataFrame({\n",
    "        'columna': [cols],\n",
    "        'unicos': [len(l_unique)],\n",
    "        'vacios': [len(l_vacios)]\n",
    "    })\n",
    "    \n",
    "    return df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "for col in df.columns:\n",
    "    aux_df = resumen_por_columna(df,col)\n",
    "    frames.append(aux_df)\n",
    "    \n",
    "df_info = pd.concat(frames).reset_index(drop=True)\n",
    "df_info['% vacios'] = df_info['vacios']/len(df)\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay datos vacíos (NaN). La columna 'diagnosis' es la que clasifica si el tumor es benigno (se definió marcar con 0) o maligno (marcar con 1)\n",
    "Al ser variables numéricas el resto de las columnas presentan valores variados. Estas representan distintos parámetros acerca de las magnitudes de dimensiones (se refiere aquí a dimensiones físicas) de los tumores estudiados. Desde los nombres de estos parámetros, se puede inferir que podría ser plausible reducir la dimensionalidad del problema ya que por ejemplo el área de un tumor se puede calcular a partir de su radio o perímetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='e2'></a>\n",
    "**2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# se normalizan las variables numéricas\n",
    "columns = df.drop(['id','diagnosis'], axis = 1).columns\n",
    "dfn = df\n",
    "dfn[columns] = StandardScaler().fit_transform(dfn[columns])\n",
    "display(dfn.head())\n",
    "display(dfn[columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='e3'></a>\n",
    "**3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Antes de elegir un método, se realiza un gráfico de correlación (ver Referencia)\n",
    "dff = pd.DataFrame(df,columns=df.drop(['id','diagnosis'], axis = 1).columns)\n",
    "corr = dff.corr()\n",
    "\n",
    "# crear máscara para eliminar diagonal y celdas sobre la diagonal\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(17,17))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr, # matriz de correlacion\n",
    "    mask = mask, # aplicar mascara\n",
    "    annot=True,  # imprimir numeros en celdas\n",
    "    cbar_kws={\"shrink\": .5}, # achicar la leyenda de la barra de colores  \n",
    "    ax = ax, # seleccionar ejes donde se ploteará\n",
    "    fmt=\".2f\" # dar formato de dos decimales\n",
    ")\n",
    "a = ax.set_ylim(ax.get_ylim()[0] + 0.5, ax.get_ylim()[1]- 0.5) # arregla un bug que achicaba el eje y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras mas claro el color hay mayor correlación entre las variables de las entradas. Mientras más oscuro menos relación.\n",
    "Hay varios conjuntos con alta correlación, se pueden explicar algunas variables a partir de otras colineales y disminuir la dimensión del conjunto de datos a analizar.\n",
    "\n",
    "En general las variables (promedio y worst) correspondientes al radio, perímetro y área de los núcleos presentan alta correlación entre sí. Se puede visualizar con scatterplot, por ejemplo, para los promedios de perímetro (**perimeter_mean**) y radio (**radius_mean**). Estas presentan una dispersión con tendencia lineal, deduciendo que se pueden asumir colineales, ayudando a disminuir la dimensionalidad del problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(7,4)})\n",
    "\n",
    "sns.scatterplot(x='perimeter_mean',\n",
    "                y='radius_mean',\n",
    "                hue = 'diagnosis',\n",
    "                palette=\"Set1\",\n",
    "                data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training = dfn[columns]\n",
    "y_training = dfn['diagnosis']\n",
    "k = 10\n",
    "columnas = list(x_training.columns.values)\n",
    "seleccionadas = SelectKBest(f_classif, k=k).fit(x_training, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catrib = seleccionadas.get_support()\n",
    "atributos = [columnas[i] for i in list(catrib.nonzero()[0])]\n",
    "atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn[atributos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from metrics_classification import summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "# Entrenamiento con todas las variables \n",
    "X = dfn[columns]\n",
    "Y = dfn['diagnosis']\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2) \n",
    "\n",
    "# Creando el modelo\n",
    "rlog = LogisticRegression()\n",
    "rlog.fit(X_train, Y_train) # ajustando el modelo\n",
    "\n",
    "predicciones = rlog.predict(X_test)\n",
    "\n",
    "df_pred = pd.DataFrame({\n",
    "    'y':Y_test,\n",
    "    'yhat':predicciones\n",
    "})\n",
    "df_s1 = summary_metrics(df_pred).assign(name = 'Todas las variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento con las variables seleccionadas\n",
    "X = dfn[atributos]\n",
    "Y = dfn['diagnosis']\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2) \n",
    "\n",
    "# Creando el modelo\n",
    "rlog = LogisticRegression()\n",
    "rlog.fit(X_train, Y_train) # ajustando el modelo\n",
    "\n",
    "predicciones = rlog.predict(X_test)\n",
    "\n",
    "df_pred = pd.DataFrame({\n",
    "    'y':Y_test,\n",
    "    'yhat':predicciones\n",
    "})\n",
    "\n",
    "df_s2 = summary_metrics(df_pred).assign(name = 'Variables Seleccionadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# juntar resultados en formato dataframe\n",
    "pd.concat([df_s1,df_s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='e4'></a>\n",
    "**4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Optimización de hiperparametros\n",
    "# Decision Tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "\n",
    "# creación del modelo\n",
    "model = DecisionTreeClassifier()\n",
    "# rango de parametros\n",
    "rango_criterion = ['gini','entropy']\n",
    "rango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150])\n",
    "param_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth)\n",
    "param_grid\n",
    "# aplicar greed search\n",
    "\n",
    "gs = GridSearchCV(estimator=model, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir resultados\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando el mejor modelo\n",
    "mejor_modelo = gs.best_estimator_\n",
    "mejor_modelo.fit(X_train, Y_train)\n",
    "print('Precisión: {0:.3f}'.format(mejor_modelo.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtc = pd.DataFrame(\n",
    "    {\n",
    "        'y':Y_test,\n",
    "        'yhat': mejor_modelo.predict(X_test)\n",
    "    }\n",
    ")\n",
    "metrics_temp1 = summary_metrics(df_dtc)\n",
    "metrics_temp1['model'] = 'Decision Tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación del modelo\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "# rango de parametros\n",
    "rango_criterion = ['gini','entropy']\n",
    "rango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150])\n",
    "param_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth)\n",
    "param_grid\n",
    "# aplicar greed search\n",
    "\n",
    "gs = GridSearchCV(estimator=model, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir resultados\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando el mejor modelo\n",
    "mejor_modelo = gs.best_estimator_\n",
    "mejor_modelo.fit(X_train, Y_train)\n",
    "print('Precisión: {0:.3f}'.format(mejor_modelo.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfc = pd.DataFrame(\n",
    "    {\n",
    "        'y':Y_test,\n",
    "        'yhat': mejor_modelo.predict(X_test)\n",
    "    }\n",
    ")\n",
    "metrics_temp2 = summary_metrics(df_rfc)\n",
    "metrics_temp2['model'] = 'Random Forest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación del modelo\n",
    "# LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# rango de parametros\n",
    "rango_criterion = ['l1','l2']\n",
    "rango_max_depth = np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150])\n",
    "param_grid = dict(penalty=rango_criterion, C=rango_max_depth, solver=['liblinear'])\n",
    "param_grid\n",
    "# aplicar greed search\n",
    "\n",
    "gs = GridSearchCV(estimator=model, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir resultados\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando el mejor modelo\n",
    "mejor_modelo = gs.best_estimator_\n",
    "mejor_modelo.fit(X_train, Y_train)\n",
    "print('Precisión: {0:.3f}'.format(mejor_modelo.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame(\n",
    "    {\n",
    "        'y':Y_test,\n",
    "        'yhat': mejor_modelo.predict(X_test)\n",
    "    }\n",
    ")\n",
    "metrics_temp3 = summary_metrics(df_lr)\n",
    "metrics_temp3['model'] = 'Logistic Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.concat([metrics_temp1,metrics_temp2,metrics_temp3], ignore_index = True)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo DecisionTreeClassifier presenta las mejores métricas en comparación a LogisticRegression y RandomForest. En general las métricas son elevadas, sin embargo al reducir la dimensionalidad a 10 columnas bajaron las métricas con respecto a no quitar columnas. Se podría haber iterado para logar obtener el numero óptimo de componentes que explican a las otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias: \n",
    "\n",
    "1. [Gráfico de correlación](https://sodocumentation.net/seaborn/topic/10634/correlation-plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
